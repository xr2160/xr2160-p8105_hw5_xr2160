---
title: "P8105_hw5_REN XIN"
output: github_document
date: "2022-11-16"
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(rvest)
library(dplyr)
library(readr)
library(readxl)
set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 6, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

##Problem 1
Solution from Jeff

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 


```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```


This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

##Problem 2

1.Describe the raw data. Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
Homicide = read_csv("data/homicide-data.csv") 
```

There are total 52,179 entries and 12 columns in Homicide database. Variables include `uid`, `reported_date`, `victim_last`, `victim_first`, `victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `lat`,`lon`, and `disposition`. 

###Step1 Cleaning the data

```{r}
Homicide_df = Homicide %>% 
  janitor::clean_names() %>% 
   mutate(
    victim_sex = as.factor(victim_sex),
    state = as.factor(state)
  ) %>% 
  unite('city_state',"city":"state", remove = FALSE)%>%
  mutate(
    city_state = 
      str_c(city, ", ", state),
    solved_or_not = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )
  )%>% 
  filter (city_state != "Tulsa_AL")
```

###Step2 summarize within cities to obtain the total number of homicides and the number of unsolved homicides

```{r}
cities_df = 
  Homicide_df %>% 
  select(city_state, disposition, solved_or_not) %>% 
  group_by(city_state) %>% 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(solved_or_not == "unsolved"),
  percent_unsolved = round(unsolved_homicides*100 / total_homicides, 1)
  ) %>% 
  arrange(desc(total_homicides))
cities_df = cities_df[!rownames(cities_df) %in% c("51") , ]
cities_df %>%
  knitr::kable()
```


Description：When processing the data we found an error in the name of the city and state of Tulsa, so I removed this data. The table shows that Chicago has the highest number of homicides, it reached 5535, and also has the highest unresolved rate (73.6%), followed by Philadelphia.


###Step3 For the city of Baltimore, MD, use the prop. test & pull the estimated proportion and confidence intervals
```{r}
Baltimore_df =
  Homicide_df %>% 
  filter(Homicide_df$city_state == "Baltimore, MD")
Baltimore_df

Baltimore_summary = 
  Homicide_df %>% 
  summarize( unsolved = sum( solved_or_not  == "unsolved"),n = n())
Baltimore_summary

Baltimore_test = 
  prop.test(
   x = Baltimore_summary %>% pull(unsolved),
   n=  Baltimore_summary %>%pull(n))
 
Baltimore_test %>% 
  broom::tidy() %>%  
  knitr::kable()
```

###Step4 run prop. test for each of the cities in this dataset

```{r}
results_df = 
  cities_df %>% 
  mutate(test_results = map2(.x = unsolved_homicides, .y = total_homicides, ~prop.test(x = .x, n = .y)),
         tidy_results = map(test_results, broom::tidy)) %>% 
  select(-test_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))
results_df
```

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point(color = "red") + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        plot.title = element_text(face = "bold.italic" )) +
  labs(title = "Figure 1: Proportion of unsolved homicides in 50 major US cities", 
       y = "Proportion of Unsolved Homicides", 
       x = "City", 
       caption = "*Note: Error bars indicate 95% confidence interval")
```
Description：According to the data in the graph, the probability of unsolved homicide varies greatly among the 50 major cities in the U.S. For example, the highest is Chicago, which even reached 73.6%, followed by New Orleans, LA with 64.9%. Richmond, VA, meanwhile, has the lowest unsolved homicide rate, at 26.3%.


###Problem 3 


```{r}
set.seed(1)
```

###Step1 Creat a function
```{r}
simtest = function(mu, sigma = 5, n = 30){
  x = rnorm(n = n, mean = mu, sd = sigma)
  a=x
  a
  result = t.test(x,mu = 0) %>%
    broom::tidy()%>%
    select(estimate,p.value)
  return(result)
}
u_iteration = 
  expand_grid(
    mu = seq(0,6,1),
    iterate = 1:5000) 
sim_results_df =
  u_iteration%>% 
  mutate(test_coef = map(mu, simtest)
  ) %>% 
  unnest(test_coef)

sim_results_df

```

###Step2


```{r}
sim_results_df %>%
  group_by(mu) %>%
  summarise(
    rej_r = sum(p.value<0.05)/n()
  ) %>%
  ggplot(aes(x = mu, y = rej_r)) +
  geom_line() +
  labs(
    x = "True μ",
	  y = "Reject_Proportion",
  )
```


Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.

Describe the relationship between effect size and power: we keep the sample size constant and the power increases in a decreasing gradient as the effect size increases


###Step3


```{r}
Total_estimates =
  sim_results_df %>% 
  group_by(mu) %>% 
  summarize(
    average_est_mean = mean(estimate)
  )

Rejected_estimates = 
  sim_results_df %>% 
  filter(p.value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(
    average_est_mean = mean(estimate)
  )

plot1=Total_estimates %>% 
  ggplot(aes(x = mu, y = average_est_mean)) +
           geom_line(aes(color = "Total")) +
           geom_point(color = "blue") +
           labs(x = "True mean", y = "Ave_estimated mean" ) +
  scale_x_continuous(breaks = 1:6)
plot1

plot2=Rejected_estimates %>% 
  ggplot(aes(x = mu, y = average_est_mean)) +
           geom_line(aes(color = "Total")) +
           geom_point(color = "red") +
           labs(x = "True mean", y = "Reject_Ave_estimated mean" ) +
  scale_x_continuous(breaks = 1:6)
plot2
```

Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

From the figure, we can see that at mu less than or equal to 4, we find that the sample mean of mu that rejects the null is not equal to the true value of the mean at less than or equal to 4. However, the two curves are close to overlapping when the sample mean of mu with rejection of null is greater than 4.







